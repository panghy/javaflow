# Flow Framework Architecture and Reimplementation Requirements

FoundationDB’s **Flow** framework is a custom actor-based concurrency subsystem built into the database’s core. Flow was created to combine the *high performance of C++* with the *maintainability of an actor model*, and to enable deterministic simulation of distributed processes in a single thread. In practice, Flow operates as a language extension to C++ that introduces asynchronous *“actor”* functions, futures, and other primitives. This document outlines the architecture and design of Flow – focusing on its actor model, coroutine scheduling, event loop integration, concurrency patterns, messaging, timing, error handling, and testing – with the goal of specifying how one might rebuild Flow from scratch in a language-agnostic way. We detail core abstractions, execution model, and internal mechanisms, then discuss limitations and potential improvements, concluding with a phased roadmap for developing a production-grade Flow-like framework in a new language. All citations refer to FoundationDB documentation, source code, or insights from its developers.

## 1. Core Abstractions and Design Patterns in Flow

**Actors and Futures:** In Flow, an *actor* is a lightweight coroutine (logical thread) that produces a result in the future. Actors communicate by sending values or messages through **futures** and **promises** rather than direct function returns. A `Future<T>` represents a placeholder for a value of type `T` that will be provided later, and a `Promise<T>` is the handle used to eventually deliver that value. An actor function always returns a `Future<T>` – meaning its result is not produced immediately, but will become available asynchronously. For example, one actor can create a `Promise<T>` and send it to another actor (possibly on a different machine); the second actor fulfills the promise (sets the value), which in turn resolves the future held by the first actor. This decoupling allows actors to execute concurrently without blocking each other.

**The `ACTOR` Keyword and `wait()`:** Flow introduces new keywords to make asynchronous code easier to write. Functions declared with the `ACTOR` modifier are special actor coroutines that can use the `wait()` operation. Calling `wait(f)` on a `Future<T>` suspends the current actor until that future is ready, allowing other actors to run in the meantime. This is analogous to an `await` in other languages – it yields control rather than blocking the OS thread. Only `ACTOR` functions can call `wait()`. The code in an actor is written in a straightforward imperative style, as if it were blocking, but under the hood the Flow actor compiler transforms it into a state machine that suspends and resumes at `wait` points. This lets developers write asynchronous logic without “callback hell,” greatly improving code clarity and maintainability.

**Promises and Message Passing:** The combination of promises and futures enables message-passing between actors. An actor waiting on a future is essentially waiting for a message (the future’s value) from whatever will fulfill the corresponding promise. This pattern is used extensively for both *in-process* and *cross-network* communication. In fact, Flow futures are location-transparent: an actor can wait on a future that will be set by a remote actor, and the code looks the same as waiting for an in-memory result. Under the covers, if the promise is sent over the network, the future will be resolved when the network reply arrives. This allows the same actor code to handle local or remote events seamlessly, which is crucial for simulation (where all “remote” messages are simulated in one process).

**Streams and `waitNext()`:** In addition to single-value futures, Flow provides **stream** abstractions for sequences of messages. A `PromiseStream<T>`/`FutureStream<T>` pair works like a producer-consumer queue: one actor can send a stream of values by repeatedly fulfilling a `PromiseStream<T>`, while another actor can asynchronously receive them via a `FutureStream<T>`. The consumer uses `waitNext(stream)` to suspend until the *next* message in the stream is available. This is useful for implementing continuous event streams or actor message inboxes. For example, many server components in FoundationDB define an interface struct containing multiple `PromiseStream` fields (one for each request type); client actors send requests by pushing to those promise streams, and the server actor processes them by looping and `waitNext()`-ing on incoming streams. This pattern effectively models actor message queues.

**Concurrency and `choose`/`when`:** Flow allows an actor to wait on multiple futures simultaneously using the `choose { when ( ... ) ... }` construct. Inside a `choose`, multiple `when` clauses each wait for a different future (or stream next value). The first one to become ready will trigger, and the actor will resume with that clause. This provides a deterministic way to wait on multiple potential events at once. For example, a server actor might use `choose` to handle whichever request arrives next among several types. The Flow runtime guarantees an *ordered and predictable* resolution of `choose` – if two events are ready at the same time, the one listed first in code wins (ensuring deterministic behavior). This is akin to `select` in Go or `Promise.race` in JavaScript, but with deterministic tie-breaking, which is important for simulation reproducibility.

**State Preservation:** Because an actor can be suspended and resumed, local variables behave differently across suspensions. Flow introduces a `state` keyword to denote variables that should be persisted as part of the actor’s state (i.e. stored in the coroutine frame). A `state` variable becomes a field in the generated state-machine class, surviving across `wait()` calls. In contrast, ordinary local variables are not automatically preserved across a suspension – the actor compiler will reset the stack for each resume. Developers must mark any data they need after a `wait` as `state` (or otherwise arrange to carry it over). For example, an actor might use `state int count` in a loop so that `count` is remembered after each asynchronous wait. The actor compiler’s transformation makes this necessary, and it’s one of the differences between Flow and regular C++ (e.g., reusing a local variable name after a `wait` requires caution). In essence, `state` variables ensure the actor’s continuity of state across yields.

In summary, Flow’s core abstractions – *actors*, *futures/promises*, *streams*, and the `wait/choose` mechanism – form a simple but powerful design: Actors are written as straightforward sequential functions that *suspend* on I/O or messages and *resume* when data is ready, passing messages via futures. This model yields highly concurrent programs that avoid explicit threading or locking, which was crucial to FoundationDB’s design goals (Flow allows many concurrent tasks but **does not use parallel threads**, see next section). A developer implementing Flow from scratch in another language should first provide equivalents of these primitives (e.g. async functions with an await, future/promise types, and a mechanism for waiting on multiple events) as the foundation of the system.

## 2. Execution Model and Coroutine Scheduling

**Single-Threaded Event Loop:** Flow uses a single-threaded, cooperative multitasking model for execution. All actors run on a single OS thread (per process) and take turns executing on an **event loop**. This design was chosen to simplify reasoning and enable deterministic simulation – true parallelism (multiple threads running at once) would introduce nondeterministic interleavings and make simulation far more complex. Thus, Flow **allows concurrency but not parallelism**: many tasks can be in progress, but at any given moment only one is actively running on the CPU. In a reimplementation, this implies using an event loop or scheduler on one thread to run all actor coroutines, rather than spawning OS threads for each actor.

**Cooperative Scheduling via `wait()`:** Actors yield control explicitly by waiting on futures. Whenever an actor calls `wait(someFuture)`, the Flow runtime saves its state and switches context to run other ready tasks. This cooperative approach relies on the programmer to insert `wait()` (or `yield`) at any point where the actor might otherwise block for a long time. In practice, almost all I/O in FoundationDB is done via futures (so any disk read, network send, etc. is a `wait` point), and thus actors naturally yield when performing I/O. If an actor needs to perform a long CPU-bound computation, it must manually break it into chunks and occasionally yield to avoid monopolizing the thread. Flow provides a utility actor called `yield()` – effectively `Future<Void> yield()` – which always suspends the current actor and immediately allows others to run, resuming the caller in the next iteration of the event loop. Developers sprinkle `Void _ = wait(yield());` calls in loops or heavy computations as an explicit cooperative preemption, ensuring that no single actor starves the others.

**Task Prioritization:** Flow assigns a **priority** to every scheduled task or actor. This is an integer (lower value = higher priority) that influences the order in which ready actors are run. FoundationDB defines multiple priority levels for different types of work – for example, critical transactions and coordination tasks use higher priority (execute sooner) than background housekeeping tasks. The event loop scheduler always runs the highest-priority ready task next. If a long-running actor yields with `wait(yield())` and specifies a lower priority, it gives a chance for higher-priority tasks to jump ahead. Moreover, tasks that wait a long time will effectively move up in priority to prevent starvation: as operations sit in the queue, their priority values are adjusted (e.g. incrementally boosted) so that older tasks eventually get served. This priority mechanism allows fine-grained control over execution order, which is crucial under heavy load to maintain low latency for important tasks. For a new implementation, one should design a similar priority queue for ready tasks and possibly adopt FoundationDB’s strategy of aging tasks to higher priority over time to ensure fairness.

**Run Loop Mechanics:** The main event loop (in FoundationDB’s `Net2` class, part of Flow) continuously pulls tasks from the ready queue and executes them. When an actor’s awaited future becomes ready (or a timer expires, etc.), that actor is put onto the ready queue with an associated priority. The scheduler then picks the next actor to run. Since the system is single-threaded, no locks are needed for shared data between actors; instead, actors cooperate by yielding frequently. Internally, the Flow event loop is integrated with the network library (Boost.ASIO, as described in the next section) – each iteration of the loop may also poll for any new network or file I/O events and turn those into ready futures. The event loop keeps track of how long each task runs and can detect *slow tasks* that don’t yield soon enough: if an iteration of the loop takes too long, Flow logs a `SlowTask` warning with the duration and even captures a stack trace if enabled. This helps developers find code that isn’t yielding appropriately. In summary, the scheduler’s job is to manage the ready list, respect task priorities, and ensure that no single actor can block the progress of the whole system. Reimplementing this would involve writing a scheduler that can track tasks (coroutines) and resume them one by one, using cooperative yielding as the context switch.

**Implicit Cancellation as Scheduling Mechanism:** One unique aspect of Flow’s execution model is how it handles task cancellation (which also affects scheduling). If the `Future` returned by an actor is dropped (no longer needed), the runtime will automatically cancel that actor’s execution. Technically, this is done by throwing a special cancellation exception inside the actor, unwinding its stack. From a scheduling perspective, a cancelled actor is removed from the run queue entirely. This prevents wasted work on tasks whose results won’t be used. The actor’s pending I/O or child actors are recursively cancelled as well by destroying any promises/futures it was waiting on. In practice, this means the scheduler doesn’t have to manually clean up or track dependencies – the C++ destructor semantics of `Future` handle it: when no one is left waiting on a future, its associated computation is pruned. A new implementation should consider a similar *structured concurrency* approach: cancellation propagating automatically when an async task’s future is ignored. This greatly simplifies resource management and ensures the scheduler isn’t cluttered with orphaned coroutines. It also ties into error handling, described later.

In summary, the Flow runtime runs a *single-threaded, deterministic event loop* that schedules ready actor coroutines according to their priority. Actors yield cooperatively at well-defined points (I/O waits or explicit yields), which requires discipline but gives the framework full control over context switches. The design avoids OS thread preemption (and its nondeterminism) entirely. Key requirements for reimplementation include: an event loop that can intermix coroutine execution with handling of I/O events, a priority-based ready queue, a mechanism for voluntary yielding, and integrated cancellation of tasks when they’re no longer needed. These features together ensure high concurrency on one CPU core while avoiding deadlocks or starvation in a long-running server.

## 3. Event Loop, Timers, and I/O Integration

**Integration with External I/O (Boost.ASIO):** In production (non-simulation), Flow builds on existing OS event mechanisms to drive its single-threaded loop. FoundationDB uses Boost.ASIO as a cross-platform abstraction for networking and file I/O events. ASIO provides asynchronous sockets (using `select/poll/epoll` under the hood) and integrates timers and file operations. Flow’s event loop (`Net2::run()`) interleaves executing actor code with pumping the ASIO event queue. Essentially, Flow registers all network reads/writes and timers with ASIO, but rather than letting ASIO’s handler callbacks run freely, Flow only processes a controlled number of events per loop iteration. In fact, Flow often calls `ios.poll_one()` or similar to retrieve at most one event at a time from ASIO, converting each into a ready `Future` that will resume the appropriate actor. By doing this, Flow **controls exactly which completed future is handled next and in what order**, instead of letting the OS or ASIO arbitrarily schedule callbacks. This control is crucial for maintaining deterministic order of events – it prevents race conditions where, say, two network packets arriving at the same time could be processed in nondeterministic order. A new implementation of Flow might use a similar strategy: employ an existing event library (like ASIO, `select`, `libuv`, etc.) for actual I/O, but funnel its events into the single-threaded actor scheduler one by one.

**Non-blocking, Futures-based I/O Interfaces:** Flow provides its own asynchronous interfaces for performing I/O, all built on futures so that they work seamlessly with `wait()`. For example, for disk access, it defines an `IAsyncFile` API for reading/writing files; methods like `read()` return a `Future<ByteBuffer>` that will be ready when the disk read completes. Under the hood, the default implementation of `IAsyncFile` uses an ASIO integration or a thread pool (on Linux it may use an *eio* thread pool or kernel AIO) to perform the actual read asynchronously. But from the actor’s perspective, it just does `data = wait(file->read(...))` and the actor will yield until the disk operation finishes. Similarly, for networking, Flow’s `FlowTransport` layer (part of `fdbrpc` library on top of Flow) provides functions to open connections, send messages, etc., which all return futures or use promise streams. *All* I/O in FoundationDB – network, disk, DNS resolution, etc. – is done through Flow’s future-based APIs, never via blocking calls. This design not only simplifies actor code but also allows Flow’s deterministic simulation (where these I/O implementations are swapped out, as we’ll see in section 6). When recreating Flow in another environment, one must implement or wrap asynchronous I/O operations into the future/promise framework so that actors can `await` them. This typically means writing thin wrappers around your platform’s non-blocking I/O (sockets, files) that fulfill a promise when the operation completes.

**Timers and Timeouts:** Flow actors can wait for time-based events using a utility actor like `delay(Double seconds)` which returns a `Future<Void>` that becomes ready after the given time elapses. In real mode, Flow uses a timer mechanism (via ASIO’s deadline timers or a similar facility) – when an actor calls `wait(delay(5.0))`, the delay function sets an ASIO timer for 5 seconds and returns a future. When the timer fires, Flow posts a completion event that marks that future ready, thus waking the waiting actor. The timer is integrated into the event loop just like I/O: the event loop checks timers and advances them. In simulation mode, Flow doesn’t rely on real time (see deterministic simulation section), but in production the `delay` uses actual wall-clock time. Flow also has a concept of a global time `now()` for the current time (in simulation this is a simulated time). For implementing a Flow-like system, providing a `delay` primitive (or more generally, a way to sleep or set timeouts on futures) is important, as many actor patterns (e.g. retry loops, periodic tasks, or giving up on slow operations) rely on waiting for time. One can use language features or libraries for timers, but ensure they tie back into the main event loop and don’t spawn threads per timer. A priority queue of timers (sorted by expiry) that the event loop checks is a common approach.

**Event Loop Structure:** In pseudocode, Flow’s main loop does roughly:

```cpp
while (!shutdown) {
    runReadyActor();                // run one actor from ready queue (respecting priority)
    pumpOneIOEvent();               // poll one event from ASIO (e.g. socket ready, timer fired)
    if (no actors ready right now) {
        wait for next IO event or timer (blocking briefly in ASIO);
    }
}
```

Because Flow often has many actors ready, it usually spins running actors until it needs to check I/O. In practice, the loop is tuned to balance CPU and I/O events. The key is that all sources of wake-ups (network, disk, timers, external signals) funnel into the scheduler as *futures that become ready*, and the loop picks them up in a deterministic order. Also, Flow can assign different priorities to different types of events; e.g. disk I/O completions might be treated as lower priority than network messages, depending on how critical they are, by tagging their futures accordingly when enqueuing.

**RPC and Message Passing Pattern:** Built on top of the basic event loop and futures, FoundationDB implements RPC between distributed components as an extension of the Flow model. Each server exposes an interface (a struct of `PromiseStream` endpoints as mentioned earlier) and registers it with the networking layer. When a client wants to call a method on that server, it sends a message (via a `PromiseStream`) containing the request parameters *and a reply promise*. The server actor, which is waiting on the corresponding `FutureStream` (via `waitNext()`), receives the request and then eventually sends a response by fulfilling the reply promise. The client, who has a `Future<Response>` from that reply promise, does a `wait()` on it to get the result. This entire exchange is done with Flow futures, so from the perspective of the actors, it’s no different than calling a local function that returns a future. The network transport (`FlowTransport.actor.cpp`) takes care of serializing the message and routing it to the correct process, but once received, it simply fulfills the target promise to hand off to the waiting actor. This pattern achieves *location transparency*: an actor doesn’t know or care if the future it’s waiting on will be delivered by a local function or a remote machine – the code is identical. For a reimplementation, this suggests building an RPC mechanism on top of the core actor framework where requests and replies are modeled as promises/futures, and the network stack is hidden behind those abstractions. The benefit is that you can then test all your distributed logic in a single process by short-circuiting the networking (which is exactly what Flow’s simulation does).

**Summary of I/O Integration Requirements:** To rebuild Flow’s event loop and I/O integration, ensure the following capabilities:

* **Single-threaded event dispatcher:** Able to handle network socket readiness, file I/O completion, and timers within one thread, feeding events into the task scheduler. You might use an existing library (like Boost.ASIO as Flow does, or libuv, or select/epoll directly) to get cross-platform non-blocking I/O. The crucial part is to integrate it such that events become ready futures in the actor system rather than immediate callbacks.
* **Future-based I/O APIs:** Wrap system calls (read, write, connect, accept, etc.) into functions that return futures. These functions should initiate the async operation and return, allowing the caller to `await` them. When the operation finishes (detected via the event loop), a promise is set to true and the waiting actor resumes. Similarly, provide a `delay(seconds)` or generic timer future.
* **Deterministic event processing:** Process at most one external event at a time between actor resumes, and/or impose a strict ordering if multiple events are ready. This is important for reproducibility. Flow even seeds the network layer’s random numbers for things like packet loss in simulation, to keep it deterministic.
* **Efficient wake-ups:** If there are no ready tasks, the loop can block on I/O (with a timeout for the nearest timer) to avoid busy-waiting. Conversely, if tasks keep arriving, the loop should not block on I/O but rather spin processing them. Flow’s design achieves this balance by intermixing polls of the I/O subsystem with checking its task queues.

By fulfilling these, the event loop becomes the heartbeat of the actor system, driving both internal (function-call style) and external (I/O) events in a unified way. Indeed, as one FoundationDB engineer described, Flow essentially provides “an evented run loop” whose specific backend (Boost.ASIO by default) is an interchangeable detail.

## 4. Debugging and Logging Tools

Developing and operating a system built on an asynchronous actor framework can be challenging, so Flow includes robust **logging and debugging aids** tailored to this model.

**TraceEvent Logging:** FoundationDB uses a structured logging system called `TraceEvent` for recording events, errors, and metrics. Actors can emit TraceEvents to a log file with various severity levels. For example, when the event loop detects a slow iteration, it logs a `SlowTask` event including the duration and the name of the task that was running. There are also periodic `Net2SlowTaskTrace` events that capture a *snapshot of the call stack* during a prolonged slow task. These backtraces help identify which actor or function was executing when the loop stalled. The log files include timestamps and type tags for each event, which engineers can analyze to trace the sequence of operations and locate performance bottlenecks or unexpected behaviors. Any reimplementation should include a similar event-logging facility, ideally with support for capturing stack traces of the running coroutine when anomalies occur. Logging every significant action or decision in the flow of the program (especially in simulation tests) is crucial for post-mortem analysis of distributed behaviors.

**Actor Context and Call Stack Tracing:** A notorious difficulty in debugging an actor-based system is that the *logical call stack* of an operation is not the same as the *physical call stack* in the runtime. In Flow, when one actor waits for another, the actual C++ call stack unwinds and later re-enters different code when the future is ready, making it hard to see the sequence of actor calls that led to a given point. Traditional debuggers will show the stack of the scheduler and callback machinery rather than a clean chain of high-level calls. To address this, FoundationDB introduced a tool called **AcAC (Actor Callstack)** that tracks the actor-level call stack at runtime. When enabled, it annotates each actor with an identifier and keeps track of which actor called (waited on) which, forming a call graph. If a severe error occurs, a TraceEvent can include an `ActorStack` field which contains a representation of the actor call stack. There are also functions to dump the current actor backtrace on demand (even in a debugger). This is immensely helpful for debugging because it tells you, for example, that actor A (perhaps a transaction commit actor) called actor B (maybe a storage write actor) which called actor C, etc., up to the point of failure – information that’s otherwise lost in a callback-based model. The implementation encodes this actor stack in a compact form (base64 in logs) that can be decoded by a tool (`bin/acac`). For a new implementation, providing some form of **logical trace** of asynchronous calls is highly recommended. This could be as simple as logging parent-child relationships of coroutine spawns, or as advanced as instrumenting the futures to carry context of who is waiting on them. The goal is to allow developers to reconstruct the sequence of events leading to an error, rather than just seeing where a generic event loop function blew up.

**Deterministic Replay and Inspection:** One of the biggest debugging advantages of Flow is that any bug found in simulation can be reproduced exactly with the same random seed. This isn’t a “tool” per se, but it’s a direct outcome of the deterministic design. When a simulation test fails (e.g., an assertion in an actor triggers or a mismatch is found), developers can take the random seed from the log and rerun the simulation with that seed to get the exact same sequence of events. This is incredibly powerful for debugging race conditions or rare timing issues – it removes the nondeterminism that usually plagues distributed systems debugging. A reimplementation should preserve this property: by using a deterministic PRNG for event scheduling, fault injection, and any timing decisions, and logging the seed, one can replay any run. The framework could even support pausing and stepping through a simulation trace event by event for introspection.

**Testing Workloads and Validation:** Tied into debugging, Flow has a built-in testing framework (discussed more in section 6) where *workload* actors generate sequences of operations and verify invariants. When a test fails, it’s usually because an invariant check (like data consistency) failed or an unexpected error was thrown. These failures are logged, and often the combination of verbose trace logs and actor stack traces will pinpoint the issue. Additionally, FoundationDB uses the `BUGGIFY` macro (see below) to surface latent bugs; any triggered Buggify event is logged so you know which fault was injected in the failing run.

**Metrics and Monitoring:** Flow’s TraceEvents also double as metrics. For example, actors periodically log events like “MachineLoad” or “TransactionMetrics” with fields that can be aggregated by monitoring tools. Since Flow schedules everything, it can easily measure things like how long certain queues are, how many actors are active, or how much time is spent in each subsystem. In a rebuild, consider integrating lightweight metrics – even just counters and timers – into the scheduling loop. For instance, Flow counts how many ASIO events have been processed, how many tasks yielded, etc., to tune performance and detect anomalies.

**IDE and Debugger Support:** Because Flow code is written in C++ with custom semantics, using IDEs/debuggers was initially tricky. The project provides an *IDE mode* where actor code is compiled as regular C++ (via some macros in `actorcompiler.h`) so that code navigation works. This doesn’t affect the running product but helps developers set breakpoints or use auto-completion on Flow code. When rebuilding in another language, this specific issue may not apply, but it’s a reminder to consider developer experience. If the new language has native async/await support, standard debugging should suffice. If not, and a custom preprocessor is used, one might need to supply tools or instructions for debugging the transformed code.

In summary, effective debugging in a Flow-like environment requires: **comprehensive logging** (with structured events and backtraces), **the ability to trace actor relationships**, and **deterministic replay** of event sequences. These features turn the normally daunting task of debugging asynchronous distributed logic into a more tractable one. When building a new system, prioritize logging every unexpected condition with enough context (actor identifiers, error codes, etc.), and ensure that your design allows reproducing issues reliably – this will save countless hours in testing and production troubleshooting.

## 5. Error Handling and Cancellation Patterns

Error handling in Flow is tightly integrated with its future/actor model. Rather than explicit error codes at each call, Flow generally uses C++ exceptions (of type `Error`, a special class in FoundationDB) to signal failures asynchronously. The actor compiler and future mechanism propagate errors in a controlled way:

**Exceptions across `wait()`:** If a future completes with an error (for instance, a disk read fails or a remote request returns an error code), any actor that does `wait()` on that future will have a C++ exception thrown at the point of the wait. In other words, `wait(f)` will either return a value (if `f` succeeded) or throw an `Error` exception (if `f` was set to an error). This allows error handling to be written in a linear style using `try/catch`. For example:

```cpp
ACTOR Future<int> fetchData(Key k) {
    state Reference<ITransaction> tr = makeReference<ITransaction>(db);
    loop {
        try {
            Optional<Value> val = wait(tr->get(k));  // may throw if transaction or network error
            if (!val.present()) return -1;
            return decodeInt(val.get());
        } catch(Error& e) {
            wait(tr->onError(e));  // built-in retry logic: returns when it's safe to retry after e
            // loop retries the transaction
        }
    }
}
```

Here, if `tr->get(k)` fails (maybe the transaction timed out or cluster is unavailable), the `wait()` will throw, and the actor catches the `Error e`. It then calls `onError(e)` (which itself returns a future that resolves after any required backoff or resetting) and waits on that to know when to retry. This pattern is how FDB’s retry loops are implemented. The key point is that **errors are propagated along futures** – the code that *sets* an error on a `Promise` doesn’t directly throw; instead, the waiting side throws when it hits the `wait`. Thus, to handle an error, you must catch it *at the `wait` site*, not within the callee that generated it. Reimplementations can choose a similar model (using exceptions if the language supports, or using `Result<T,Error>`-style futures). The model should ensure that an error in an async task automatically causes any dependent `await` to signal an error.

**Automatic Cancellation:** As touched on earlier, Flow has a powerful automatic cancellation mechanism. *Cancelling an actor* means terminating its execution because its result is no longer needed. In Flow, this happens implicitly when the last reference to its `Future` is dropped. For example, if actor A starts actor B (calls an `ACTOR` function B and gets a Future back) but then encounters an error and decides not to wait for B’s result, it may destruct the Future<B> (perhaps by leaving scope or assigning a new value). In Flow’s design, this triggers the cancellation of actor B. The actor runtime does this by generating a special `error_code_actor_cancelled` exception inside B’s context (the `wait()` that B is currently on will throw this), unwinding B’s stack and cleaning it up. As B unwinds, if B was waiting on other futures (say B had spawned a child C and was waiting on it), those futures are also cancelled in turn by the destruction of their promise references. This cascades down, so an entire subtree of actors can be cancelled simply because the top-level caller gave up on the result. Remarkably, this means *most Flow code does not need to write explicit cancellation logic* – it happens by design. As David Scherer (FDB co-founder) noted, recursive cancellation of a call tree “works effortlessly and correctly” in Flow, to the point that “there is so little code dealing explicitly with cancellation that you could miss that it is a feature”.

In a new implementation, achieving this kind of transparent cancellation is a critical feature. Some languages with garbage collection might not immediately free an unused future, so you may need an explicit cancellation call. However, you can mimic Flow’s pattern by reference counting futures and triggering cancellation when count drops to zero, or by using structured concurrency primitives (in languages that support it) where cancelling a parent task automatically cancels children. The cancellation should propagate deeply: if task A is waiting on B and B on C, then cancelling A should ultimately cancel C. This prevents wasted work and is important for building timeouts and fail-fast behavior. For instance, FoundationDB uses this in scenarios like: if a transaction is cancelled (by the client or because it’s retried), all the async work it spawned is cancelled to avoid doing useless work.

**Error Codes and Retriable Errors:** FoundationDB defines a set of error codes (like code 1007 = `transaction_cancelled`, 1009 = `request_future_version`, etc.), each represented by an `Error` object. Actors often catch specific errors to implement retry or alternate logic. For example, a transaction commit actor catches `not_committed` errors to retry, or a coordination actor might catch a `coordinators_changed` error to refresh cluster state. In a Flow reimplementation, one would likely have a similar set of well-known error conditions (e.g., a cancellation exception type, a timeout exception type, etc.) and write actor code with try/catch around waits to handle those. If using exceptions, it’s straightforward; if not (say, using result types), then each await would return a result and you’d propagate errors manually – but that can clutter code, so the exception model (or throwing futures in some languages) is preferred for clarity.

**No Uncaught Exceptions in Actors:** Typically, if an actor function doesn’t catch an exception that is thrown, that exception will propagate out of the actor’s future, meaning any actor waiting on it will then get that exception. If it propagates all the way to the top (no one catches it anywhere), in FoundationDB it often indicates a fatal error (like an internal invariant failed) and may terminate the process or at least the simulation. But most expected error paths are handled gracefully by catching and converting to alternate flows (retries, aborts, etc.). In a robust design, consider where uncaught errors go – perhaps to a global error handler or to logs. Flow’s simulation will flag uncaught errors and fail the test, which is what you want in testing.

**Timeouts and Cancellation Patterns:** Many actor patterns involve timeouts (e.g., try something, and if it doesn’t finish within X seconds, cancel it and do something else). In Flow this is done by racing a delay. For instance, you can write:

```cpp
choose {
    when (Value result = waitNext(stream.getFuture())) {
        return result;
    }
    when (Void _ = wait(delay(5.0))) {
        throw operation_failed(); // or some timeout error
    }
}
```

This will wait either for the next stream value or 5 seconds, whichever comes first. If the delay wins, it throws an `operation_failed` error (which might be caught by caller to handle a timeout). The losing branch in a `choose` is implicitly cancelled (the promise for it is discarded), so if the stream value arrived at 6 seconds, it gets cancelled by the timeout. This shows how futures and cancellation interplay to handle timeouts. We’d want to replicate this ability: effectively the equivalent of `Promise.any` with cancellation of losers.

**Resource Cleanup:** Because Flow relies on RAII (destructors) for cleanup, when an actor is cancelled or completes, it automatically frees any resources it was holding (file handles, memory, etc.) as long as they are in destructors. Flow’s `Promise` and `Future` destructors ensure they remove themselves from any lists (for example, a `PromiseStream` will remove a listener if the future is cancelled). This design minimizes explicit cleanup code. In a garbage-collected implementation, you’d need to be more careful to explicitly unregister things on cancellation.

In summary, the Flow framework demonstrates a *propagating failure model*: errors and cancellations flow along the same paths as data, in reverse. A future that carries a failure causes an exception at the wait site; a cancelled wait causes an exception back at the callee. This greatly simplifies writing robust async code, as each actor can be written assuming “happy path” logic, with catch blocks for well-defined failure conditions, and dropped tasks will quietly terminate without leaks. A reimplementation should strive for similar semantics, whether using exceptions or an equivalent mechanism: the ability to *throw across an await*, and automatic *cascade cancellation*. These features make complex concurrent workflows (like multi-retry transactions, or parallel requests with cancellation of the slower ones) much easier to implement correctly.

## 6. Deterministic Simulation Testing and Failure Injection

One of the most distinctive features of Flow (and FoundationDB generally) is its **Deterministic Simulation** system. This was a primary motivation for creating Flow and is regarded as FoundationDB’s “secret sauce” for reliability. Rebuilding Flow should include recreating this simulation capability, as it drastically improves testing of distributed systems.

**Single-Threaded Simulation of a Distributed Cluster:** Flow’s design allows an entire FoundationDB cluster (with multiple processes/roles, networking, and disk I/O) to be simulated in a single-threaded program. This is done by replacing all the real-world interfaces (network, disk, clock) with *simulation shims*. In simulation mode, instead of using Boost.ASIO and real sockets, Flow uses in-memory queues to represent network connections. Instead of actual file I/O, it uses a simulated disk that can optionally be backed by an in-memory representation or even a real file but with controlled scheduling. And instead of the system clock, it uses a *virtual clock* that it advances itself.

The event loop in simulation is different: it doesn’t tie into OS I/O at all. Rather, it manages a priority queue of “scheduled events” keyed by simulated time. When an actor in simulation does `wait(delay(x))`, the runtime will schedule that actor to resume at *simulated time = current\_time + x* by putting it into the priority queue. When there are no immediate tasks to run, the simulation loop will advance the current time to the next event’s time and resume it. This way, time jumps directly to when something happens, allowing the simulation to run much faster than real time for idle periods. For I/O, the simulation uses simplified models: for example, a disk write might be executed synchronously (since we can do it in-memory), but the simulator will insert a *delay* to simulate disk latency and throughput limits. A network message send might simply enqueue the message in the recipient’s queue and schedule a future event after some simulated network delay before the recipient actor is notified.

**Determinism and Pseudorandomness:** The key is that all randomness in simulation is controlled by a single pseudorandom number generator (PRNG) with a known seed. When simulation starts, it takes an arbitrary random seed (or a fixed one for replay). All components (network latency, disk delays, failure injections, etc.) draw from this PRNG. Because everything is single-threaded and events are processed one at a time, the random choices occur in a repeatable order given the same seed. Thus, the entire simulation becomes a pure function of the seed, yielding deterministic behavior. If a test fails with seed 12345, running again with seed 12345 will produce the same sequence of events, down to the exact interleaving of messages and failures. This determinism is what enables both rigorous testing (you can stress the system with random faults) and perfect reproducibility of bugs.

**Fault Injection with BUGGIFY:** Flow includes a macro `BUGGIFY` sprinkled throughout the FoundationDB codebase to inject random faults and unusual conditions during simulation. When simulation is enabled, `BUGGIFY` statements become active based on the random seed. Essentially, at the start of a simulated run, a subset of `BUGGIFY` flags are turned on (randomly), and each `BUGGIFY` instance has a small probability (e.g. 5%) of triggering each time it’s reached. These inject faults like causing a normally one-off operation to throw an error, or introducing an extra delay, or dropping a packet. Over many runs with different seeds, various combinations of `BUGGIFY` lines will fire, exploring edge cases. For example, there might be a BUGGIFY in the code path for disk flush that occasionally makes it fail with an IO error, or one that introduces corruption in memory to simulate bit flips (in memory simulation). The idea is to *proactively push the system toward failure scenarios* to ensure they are handled correctly. The simulation will run thousands of random scenarios, effectively fuzz-testing the distributed system. When rebuilding, a `BUGGIFY`-like mechanism (or generally, fault injection hooks) is vital. One should design the system to allow injecting failures in I/O operations, network messages, timers, etc., under controlled conditions. Toggling fault injection can be tied to a global random stream as FDB does, or by a config for specific tests.

**Testing Workloads:** In simulation, FoundationDB doesn’t just run the core database actors; it also runs special *test workload actors*. These are user-level scenarios (like “start and stop nodes”, “generate random transactions”, “verify data consistency after random splits/merges”) that execute concurrently with the system. Each workload is an actor or a set of actors that perform some coordinated activity. For instance, one workload might continuously open and close connections to simulate clients coming and going, another might kill and restart certain server roles at random times, and another might insert and read data checking for consistency. These run as part of the simulation, using the same deterministic scheduler. At the end of a simulation run, the workloads have `check()` methods to verify that invariants hold (e.g., no data corruption, all data written was eventually readable, etc.). The simulation thus can test not only that the system doesn’t crash, but also that it meets its correctness conditions through all these events. For a new system, you’d want to create a similar harness: the ability to launch multiple “background” test actors that simulate clients or failure events, and then assert conditions.

**Simulation vs. Real Mode Implementation:** A crucial architectural point is that Flow achieves simulation by abstracting the hardware interfaces. There is an `INetwork` interface in Flow, with two implementations: one for real (epoll/ASIO-based, multi-process) and one for simulation (single-process, timejump, etc.). Similarly, `IAsyncFile` has a normal implementation (using threads or AIO) and a simulated one. The code that uses these interfaces (the bulk of the database logic) is written *to the abstraction*, so it doesn’t know whether it’s in simulation or not. For example, to open a file you call `IAsyncFileSystem::filesystem()->open(path)`, which will dispatch to either a real file open or a simulated file open depending on a global mode. This means the same actor code runs in both environments. To replicate this, you should architect your framework such that all environmental dependencies (timers, networking, disk, randomness, threading) can be swapped out or controlled. Perhaps define interfaces or use dependency injection to provide a *simulation context* vs *real context*. This adds complexity but pays off in testability.

**Simulated Clock and Ordering:** In simulation, time is a controlled variable. Flow’s simulated clock starts at 0 and only advances when the scheduler explicitly advances it. Operations that would block (like sleep or waiting for I/O) instead schedule themselves to resume at a future simulated time. The scheduler picks the event with the smallest timestamp from the priority queue to run next, and sets the current time to that event’s timestamp before running it. This creates a *logical timeline* of events. Because there’s no parallelism, if two events are scheduled for the exact same time, their order is deterministic (likely FIFO or in some defined order of insertion). In practice, the simulator may avoid scheduling two things at exactly the same time to maintain clarity (introducing tiny epsilon differences). The absence of true parallel operations means we simulate concurrency by interleaving events in a single thread. This is powerful because it converts what is normally a nondeterministic interleaving problem into a deterministic one (given a random seed controlling the schedule).

**Failure Simulation:** The simulator is not limited to software faults; it also simulates hardware and network faults. For example, the network layer can simulate lost or reordered packets, or even “link down” events that drop all messages between certain simulated processes. Disk simulation can flip bits or throw I/O errors. The FoundationDB simulation can simulate process crashes by simply stopping the actors corresponding to that process and discarding their state, then perhaps later restarting them (all within the one OS process). Since all state is in memory, a “crash” is just dropping some in-memory context and starting fresh ones – fully under the control of the sim. This is how things like master failover, data move, etc., are tested without needing multiple machines. If rebuilding Flow, consider how you will simulate separate “nodes” or “processes”. One way is to instantiate multiple copies of your server actor stack, each with its own identity, and have the simulated network deliver messages or drop them accordingly. You might maintain separate logical threads-of-execution in the simulation for each simulated process (though still run them on one real thread sequentially). FoundationDB’s simulation effectively multiplexes many *virtual processes* onto one real thread by assigning each actor a “process ID” context and using that in the network layer to decide routing. They can even simulate different machine speeds by giving different processes different CPU budgets (in simulation, that might mean certain processes have extra delays on their tasks to mimic slower CPU).

**Massive Testing Scale:** Over the years, FoundationDB has reportedly executed an enormous amount of simulation testing – trillions of CPU-hours in simulation runs, covering countless scenarios. The outcome is a very hardened system. If we reimplement Flow and use simulation, we should plan for similar brute-force testing: you’d write dozens of scenario workloads (transaction heavy, machine failures, network partitions, etc.), then run thousands of random combinations of them with fault injection overnight. Simulation finds corner cases that are nearly impossible to hit with manual testing.

In conclusion, **deterministic simulation** is the capstone feature of Flow. To include it in a rebuild, plan the architecture in dual modes from the start: one mode where you plug into real world, and one where you drive everything internally. Include a global deterministic scheduler, a controllable clock, and abstract interfaces for all nondeterministic operations. Also create a rich fault injection system to really stress the system. The benefit of doing all this is huge: you gain confidence in the correctness and fault-tolerance of your system in a way that few other testing methodologies can provide. As a bonus, the simulation mode can also be a development tool – you can run a whole “cluster” on your laptop in one process for quick experiments.

## 7. Compilation and Code Transformation (`ACTOR` Keyword and Coroutine Mechanics)

Flow is often described as a “language” or a “compiler” on top of C++. In reality, Flow is implemented via a source-to-source transformation: a custom *actor compiler* processes Flow-annotated C++ code and produces pure C++ that uses callbacks and state machines to implement the coroutine behavior. Understanding this compilation model is important both for appreciating Flow’s constraints and for deciding how to implement actors in a new environment.

**Preprocessor to Compiler Pipeline:** Flow code lives in files usually named `Something.actor.cpp` (by convention). These source files contain the `ACTOR` functions with `wait()` calls and other Flow-specific keywords. When building FoundationDB, a special compiler (written in C# and invoked via Mono) runs to translate these `.actor.cpp` files into temporary `.g.cpp` files (or similar) that contain equivalent C++ code without any `wait()` or `ACTOR` – instead, they contain generated classes and functions representing the coroutine state machine. Then the normal C++ compiler (clang or MSVC) compiles those generated files. The Flow actor compiler adds about 10 new keywords (like `ACTOR`, `wait`, `state`, `choose`, etc.) to C++. It uses a simple syntax (the code still looks a lot like C++ with blocks and braces) but with these keywords that it understands and C++ doesn’t. Think of it as a transpiler that backports C++20 coroutines to C++98 era compilers.

For example, an actor like:

```cpp
ACTOR Future<int> asyncAdd(Future<int> f, int offset) {
    int value = wait(f);
    return value + offset;
}
```

would be translated under the hood into something like:

```cpp
struct asyncAdd_state {
    // state variables
    Future<int> f;
    int offset;
    int value;
    // promise to hold result
    Promise<int> returnPromise;
    // resume function
    void operator()(bool resume = false) {
        if (!resume) goto Line1;
        // resume point
        goto Line2;
      Line1:
        // initial suspend: wait on f
        f.then([this](){ (*this)(true); });
        return;
      Line2:
        value = f.get();
        returnPromise.send(value + offset);
        delete this;
    }
};
Future<int> asyncAdd(Future<int> f, int offset) {
    auto* s = new asyncAdd_state{f, offset};
    auto future = s->returnPromise.getFuture();
    (*s)(); // start execution until first wait
    return future;
}
```

*(The above is illustrative pseudocode; the actual generated code is more complex.)* The idea is that each `ACTOR` becomes a struct with the local variables as fields (`state` variables ensure persistence), and with an `operator()` or method that implements a part of the function. Every `wait` is turned into a split: scheduling a callback (`.then(...)`) to resume the actor at that point, and returning control to the event loop. The actor’s execution is thus broken into segments separated by yields.

**Handling of Flow Constructs:** The actor compiler must handle `choose`/`when` by turning them into if/else or switch constructs that check the readiness of multiple futures. It likely transforms `choose` into something that sets multiple callbacks and whichever future triggers first cancels the others and continues the actor from the appropriate branch. The `state` variables are simply lifted to struct fields (the compiler has to identify them). The `loop` constructs in actors become gotos or while loops in the state machine, etc.

**Switch and Variable Limitations:** Certain C++ constructs don’t translate cleanly. For instance, a `switch` statement that spans a `wait` is problematic, because the state machine might need to resume inside the switch. The Flow documentation explicitly warns not to use `switch` with `wait()` (or to put a `wait` inside a switch without braces). Similarly, as mentioned, redeclaring a local variable after a wait is not allowed unless you re-scope it, because the C++ compiler in IDE mode would see it as a redefinition in the same block. The actor compiler works around these issues by certain naming or scoping tricks, but Flow programmers need to be aware of them (or just always use `state` for anything that persists and avoid tricky switch cases). In a redesign, if using a language with built-in coroutines (like Python `async` or modern C++ `co_await` or Rust `async`), you don’t need a custom compiler; the language handles the transformations. But if not, you might consider implementing a similar source transform or using a library to create coroutine state machines.

**Performance of Generated Code:** The generated code essentially uses heap allocation for the actor struct and uses function pointers (callbacks) for resumption. This is quite efficient – it’s similar to a hand-written async state machine. It avoids the overhead of stackful coroutines (which require saving a full stack context) by instead only storing the necessary state. The Flow team has been satisfied with its performance; they note it’s “nice and fast at run-time” even if the generated code is “very ugly” to look at. One reason it’s fast is that there’s no scheduling or context-switching beyond function calls and a bit of pointer manipulation. When a future is ready, the overhead to resume the actor is just invoking a callback. The determinism benefits outweighed the complexity of maintaining this custom compiler.

**Compiler Implementation:** Notably, the Flow actor compiler itself is written in C# and was originally developed early on (they mention it took the first two weeks of development). It’s essentially a parser for a subset of C++ that outputs C++. This choice might seem odd, but possibly the team was familiar with C# and its parsing libraries, and it was quicker to implement. For a new project, one could implement the actor logic using modern language features instead – for example, if using C++20, you can avoid a separate compiler and use `co_await` to implement similar coroutines (Alex Miller mentioned they have an eye on replacing the actor compiler with C++ resumable functions eventually). In a language like Kotlin or C#, you have async/await natively. In Rust, you have `async/.await` but might need to ensure single-threaded executors for determinism. If using a language without such support (or wanting fine control), writing a small source-to-source transformer or using macros could be an approach.

**Build Integration:** Flow’s build (CMake) is set up so that `.actor.cpp` files are first run through the actor compiler to produce `.actor.g.cpp` files. There’s also the `actorcompiler.h` which is included at the end of actor files to trigger certain macros. In *IDE mode*, those macros are defined such that `ACTOR` is basically nothing (so the IDE sees a normal function signature returning `Future<T>`), and `wait(x)` is defined as just calling a dummy function (so it parses), etc.. This allows the code to compile (in a non-working way) for intellisense, but in actual build, the actor compiler *ignores* those and does its own processing. If reimplementing, you might not need such complexities if your environment supports the constructs.

**Language Agnostic Considerations:** If the target language natively supports coroutines/async (e.g. Python’s `async def`, JavaScript’s async/await, Kotlin’s coroutines, etc.), you don’t need a source translator at all. You’d simply write actors using those and ensure your runtime provides the needed scheduling and determinism. If the language doesn’t, you might implement a minimal state machine library. For instance, in C, you might use Duff’s device or macros to implement something like Protothreads. But a full separate compiler like Flow’s might be overkill unless performance or integration demands it.

**Compilation Pipeline Requirements:** For a new design:

* If using custom transformation: define the syntax (maybe reuse Flow’s to leverage their design, which is proven), implement a parser/translator, integrate with build.
* Or leverage an existing coroutine feature.
* Ensure that whatever approach allows hooking into the scheduling (e.g., you might need to implement your own Executor for async tasks to control order).
* The end result should produce efficient, debuggable code. Flow’s approach had the downside of making debugging tricky (as we saw), but it gave them full control over the execution.

The main *requirement* here is to have a mechanism that allows writing asynchronous code in a linear style and get it executed cooperatively. Whether via a custom compiler or native language features, that needs to be in place. The rest of the system (event loop, etc.) then works with these coroutines.

## 8. Limitations of the Current Design

While Flow has been highly successful within FoundationDB, the designers themselves have noted a few limitations and “pain points” in the current implementation. When contemplating a reimplementation, these are areas one might improve or approach differently:

* **Not a General-Purpose Library:** Flow was built with very specific goals and assumptions about distributed systems, and it is tightly integrated into FoundationDB. It is *not* a drop-in async library that can be easily reused in other projects without pulling in a lot of baggage. The architecture is opinionated: single-threaded, custom networking, etc., which might not align with every use case. In practice, Flow has rarely been used outside of FoundationDB (there was an attempt to factor it out into a separate repo, but it’s not mainstream). This limited reuse is a drawback if one wanted to leverage Flow’s strengths in other contexts. A new design could aim to be more modular, so that the core event loop and actor runtime could be adopted elsewhere without depending on FDB-specific code (like its serialization or specific errors).

* **Manual Memory Management / C++ Footguns:** Being in C++, Flow requires careful memory management of actor state and messages. While C++ is deterministic in destruction (which Flow leverages for cancellation), it also means issues like dangling pointers or memory leaks can happen if mistakes are made. The Flow actor compiler generates a lot of dynamic allocations (each actor state on the heap) – though in practice this overhead is acceptable, it’s something to consider. A safer language (Rust, or a GC language) might simplify memory concerns at the cost of losing deterministic destruction timing (affecting the cancellation-by-destruction model). Flow’s design cleverly uses C++ RAII for correctness, but it’s an extra cognitive load for developers who must manage reference counts (FDB uses reference-counted types `Reference<T>` extensively for objects like transactions, etc.). Newer languages might allow eliminating entire classes of memory errors.

* **Learning Curve and Tooling:** New developers to FoundationDB face a learning curve grokking Flow. It looks like C++ but has its own rules and gotchas, which can lead to confusion if one writes code that compiles in IDE (where `wait()` is a no-op macro) but fails in the actor-compiled build. Debugging transformed code can be non-intuitive. This is somewhat mitigated by documentation of caveats and by the existence of modern coroutine analogs (so experienced devs can map the concepts). Still, Flow is essentially an internal domain-specific language – that’s a maintenance burden. If reimplementing in a language with first-class coroutine support, this burden would reduce as you’d rely on known constructs rather than a custom one.

* **Single Thread Bottleneck:** By design, a Flow process runs all actors on one thread (except some low-level threads for specific tasks like flushes). This means a single FDB process is limited to one core’s worth of actor work. FoundationDB addresses multi-core scaling by running multiple processes (usually one per core) in a cluster. This works for their deployment model (and even on one machine, you can run multiple `fdbserver` processes pinned to different cores). However, it is a limitation if you wanted to utilize many cores within one process. Flow does not parallelize a single workload across threads. In modern servers with dozens of cores, spawning processes is fine but can have overhead or complexity (inter-process communication instead of in-process calls). A future version of Flow might consider allowing multiple worker threads while still retaining deterministic simulation (which is a hard problem – one solution could be deterministic, cooperative threading per core plus a deterministic merging of logs for simulation). But as of now, parallel CPU work must be offloaded (e.g., storage engine disk queue uses multiple threads to fill IO pipes, but those are somewhat peripheral). So, one limitation: if your use case needed lots of parallel computation (not just I/O concurrency), Flow would not directly leverage all cores without spawning parallel Flow subroutines in separate processes.

* **No True Actor Isolation:** In actor models like Erlang’s, each actor has its own isolated heap and only communicates via messages, which provides fault isolation – one actor crashing doesn’t bring down others (it can be supervised/restarted). Flow’s actors are just coroutines in the same address space; if one throws an uncaught exception or corrupts memory, it could crash the whole process. There isn’t an actor supervisor hierarchy as in Erlang OTP. FoundationDB instead leans on its simulation tests to catch issues rather than isolating them at runtime. In a new design, one might consider adding more isolation or at least structured supervisor relationships (for example, grouping actors by roles and handling failures in a more contained way). However, introducing isolation could conflict with the extremely tight integration Flow has (actors frequently share data structures or pass raw pointers, etc.). Achieving Erlang-level isolation in C++ would be tough; in a higher-level language it might be easier, but still, determinism with preemptive isolation is tricky.

* **Strict Requirements for Determinism:** Flow’s determinism comes at a cost of not being able to use certain system features freely. For instance, any asynchronous operation must go through Flow’s controlled interfaces – you can’t spin up a background thread that does something nondeterministic in simulation. This means integrating third-party libraries can be challenging if they internally start threads or do their own async logic. The Flow ecosystem had to implement a lot in-house (e.g., their own DNS resolution in a Flow way, etc.). This is a limitation in terms of ecosystem: it’s not trivial to use, say, an off-the-shelf HTTP client library inside Flow unless you modify it to fit Flow’s model. A future Flow in another language might try to mitigate this by providing adaptors or running external operations in a way that’s accounted for in simulation.

* **Complexity of Actor Compiler:** Maintaining a custom compiler (written in another language) in the build chain is cumbersome. While it has only had a couple of bugs historically, it’s technical debt. As C++ evolves (introducing native coroutines), the impetus to replace the actor compiler grows. The team has indicated interest in overhauling Flow by using modern C++ coroutine support, which would remove the need for their compiler and possibly improve compatibility with tools. So, the limitation here is *maintenance overhead* for the custom solution. For a new implementation, it would be preferable to avoid inventing a new language if possible and instead use robust language features.

* **TLS and Platform-specific Code:** There were notes that certain parts, like TLS (encryption) integration, are not in great shape in Flow’s current form. This is a smaller detail, but indicates that some peripheral components (maybe because they don’t fit cleanly into the async model, or just lack attention) could be improved. For example, handling SSL sockets within Flow might have issues. Similarly, some filesystem code is ripe for cleanup. These are more like engineering backlog items than fundamental design flaws, but any reimplementation could aim to address such things from scratch (e.g., ensure that secure communication and file access patterns are well-supported in the new model).

* **Steep Debugging When It Breaks:** Despite all the tools, debugging a live running FoundationDB node can be tough if you don’t have simulation. If something goes wrong in production that wasn’t caught in simulation, you have limited options: logs and maybe core dumps. Attaching a debugger to a live Flow program and trying to understand what all the actors are doing is not straightforward (the call stack doesn’t show you all actor states without AcAC). This is somewhat inherent to any highly asynchronous system, but Flow’s use of exceptions for flow-control and callbacks can make core dumps hard to interpret. So, one could say a limitation is **observability** of a running system – albeit addressed by logs and metrics to a large extent.

To sum up, Flow’s limitations include its **niche, tightly-coupled nature**, the **lack of multi-thread scaling within a process**, the burden of a **custom preprocessor**, and some **ergonomic challenges** for developers. None of these negate its benefits, but they are points to reflect on when designing a new system. The next section will consider how we might improve or avoid these issues in a redesign.

## 9. Areas for Improvement in a Rebuilt Flow

If we were to rebuild Flow from scratch in a modern context, there are several improvements and new features we could consider, informed by the limitations above and advancements in technology since Flow’s inception:

* **Leverage Native Coroutine Support:** A top priority would be to use language-native async/await or coroutine facilities instead of a custom actor compiler. Many languages now have this (C++20 `co_await`, Rust `async/.await`, Python `async def`, etc.). Using these would immediately improve compatibility with tools and reduce maintenance overhead. For example, rewriting Flow’s logic using C++20 coroutines could eliminate the entire C# actor compiler step. This change could also enable easier integration of third-party components (since they might also use futures or awaitables that can interop). **Improvement:** Simplify the implementation by removing the custom preprocessor/compiler – rely on language features for coroutine transformations.

* **Multi-threaded Execution Options:** While determinism is easiest with single-thread, a new Flow might explore running multiple scheduler threads in parallel for different groups of actors when in production mode (but still have a deterministic single-thread simulation mode). For instance, one could partition actors by some domain (perhaps by “virtual node” or by role) and run each partition on a separate thread in production for throughput, but in simulation run them time-sliced on one thread. Achieving identical results in both modes is complex, but maybe possible if threads don’t interact except through well-defined channels that simulation can linearize. Alternatively, one might allow multiple threads in production without deterministic interleaving (accepting simulation won’t cover data races then). Given the value of determinism, a safer approach is probably to keep single-thread per process, but it might be worth investigating if multiple deterministic threads could be orchestrated (there is research in deterministic multi-threading). **Improvement:** Increase parallelism support – perhaps by allowing optional multi-thread execution or by making it easier to scale out on multi-core via a unified process (this is an open design question).

* **Modularize Flow as a Library:** Design the new Flow so that it can be used independently of FoundationDB. This means clearly separating the actor runtime, networking, and event loop from database-specific logic. For example, the new Flow could be packaged as a standalone library where users can create a Flow scheduler, spawn actors, and use Flow’s deterministic simulation harness in their own projects. This would involve generalizing any assumptions (like removing direct ties to FDB’s config classes, etc.). The benefit would be a wider adoption and contribution pool. **Improvement:** Increase reusability and modularity, turning Flow into a general-purpose actor framework with deterministic testing capabilities.

* **Enhanced Actor Abstractions:** Introduce some higher-level actor patterns that Flow currently doesn’t provide out-of-the-box. For example, a notion of *supervised actors*: ability to spawn an actor under a supervisor that gets notified if it fails (uncaught error). Or built-in *timeout combinators*: maybe more convenient syntax to cancel after a timeout. Or *actor pools* (groups of actors working on tasks). Flow code currently implements these patterns manually (with loops and chooses), but a new library could offer helpers. Also, possibly incorporate ideas from structured concurrency – ensuring when you spawn sub-actors, they are joined or canceled when the parent exits unless explicitly detached. Flow already does some of that (child futures canceled with parent), but more structured relationships could be made explicit.

* **Better Debugging/Introspection Tools:** While Flow’s AcAC is a great addition, a new design could plan for introspection from day one. For example, provide an API to enumerate all live actors and their statuses (waiting on which future, etc.) at runtime. Perhaps allow querying of an actor’s stack in-memory without needing special compile flags (if using a language with reflection or debug info accessible). One could integrate with tracing systems to log not just events but spans (actor X from start to end, including what awaited on what). If we use a managed runtime (like JVM or .NET), maybe we could leverage their profilers to visualize actor lifetimes. **Improvement:** First-class support for actor debugging, like being able to attach a REPL or web dashboard to the running system to inspect actor states (in simulation or even live).

* **Simplified Cancellation and Timeout API:** Flow’s automatic cancellation is nice but implicit (via dropping futures). A new design might also allow more explicit cancellation controls. For instance, a `cancel()` method on a future or a `CancellationToken` that can be passed down (similar to .NET’s token). This would give more flexibility, especially in languages where relying on destructors isn’t viable. The automatic propagation should still exist, but explicit triggers can help orchestrate cancellation in complex scenarios. **Improvement:** Provide structured cancellation tokens or contexts, so that code can cancel sub-tasks in a group or with a reason, rather than solely by dropping references.

* **Integration with Modern Networking/IO frameworks:** Instead of a custom network layer (FlowTransport), maybe integrate with an industry-standard RPC or messaging system, as long as it can work with our futures. But careful: many frameworks spawn threads or are nondeterministic. Perhaps build on a lower-level event library like `libuv` or something like `io_uring` on Linux for efficiency. Since 2013, Linux and others have better async IO options. If writing in Rust, use `tokio` but with a single-thread scheduler for determinism. **Improvement:** Use state-of-the-art async IO to possibly reduce latency or overhead (Flow’s networking is good but might be improved, e.g., using modern syscalls or zero-copy).

* **Testing and Simulation Enhancements:** Even though Flow’s simulation is great, there might be improvements. For example, more fine-grained control over random fault probabilities (perhaps adaptive testing that focuses on recently changed code paths more). Or the ability to attach a model checker or invariant checker systematically. Possibly integrate with tools to visualize event schedules or automatically minimize failing schedules (shrinking the random seed space that reproduces a bug). These are more advanced testing features that could be layered on. **Improvement:** Build even better failure diagnostics into simulation – like when an invariant fails, automatically dump the sequence of events (maybe in a trace file) and create a reduced test case.

* **Performance Optimizations:** Although Flow is performant, there are always possible tweaks. A new implementation could aim for fewer allocations (maybe pooling actor structures or using stackful but segmented stacks to avoid per-actor heap alloc). Or using lock-free queues for ready tasks (Flow likely already does something efficient). Another area: memory overhead per actor – Flow actors can be somewhat heavy (hundreds of bytes each). If one wanted millions of actors, perhaps a lighter weight representation (though FDB doesn’t need that many, since it’s bounded by number of client transactions etc.). **Improvement:** Optimize the runtime to support larger numbers of actors with less overhead and possibly consider idle actor memory footprint (maybe compress state when idle? Not trivial though).

* **Cross-Language Bindings:** If Flow were standalone, one could consider providing bindings to other languages or making it possible to write actors in different languages that interoperate. For instance, one might allow writing certain actors in Python that interact with ones in C++. But determinism might break if two languages are involved with different runtimes. Perhaps out of scope unless the entire system moves to a single high-level language.

* **Better Documentation and Patterns:** Not a technical change, but an improvement area is documenting Flow’s best practices and patterns. The current docs cover basics, but a new project could include a cookbook of common concurrency patterns (e.g., pipeline actors, scatter-gather, etc.) implemented in Flow style, to help users. Also, if reuse is a goal, then thorough reference docs and maybe a community around it would be beneficial.

In essence, a rebuilt Flow should **embrace modern language features**, **improve developer ergonomics (debugging, cancellation, patterns)**, and **possibly extend its concurrency model** to be more flexible (without sacrificing its unique deterministic testing ability). The core idea of cooperative actors with deterministic scheduling should remain, as it’s central to its value.

## 10. Roadmap for Building a Production-Grade Version in a New Language

Building a Flow-like framework from scratch is a significant project. It’s wise to approach it in **phases**, validating at each step that the core principles hold (especially determinism and performance). Below is a suggested phased roadmap:

**Phase 1: Core Futures and Actors** – *Establish the basic async infrastructure.*

* Implement a `Future<T>` and `Promise<T>` type (or use the language’s built-in ones if suitable) that can be used for synchronization between tasks. Ensure that the Future can carry a result or an error, and that it supports continuation callbacks (or `await`).
* If using a language with native coroutines, set up a way to launch an async function (actor) and obtain its future. If not, implement a minimal actor system: perhaps define an `ACTOR` macro or a pattern to write state-machine code.
* Create a simple event loop that can schedule tasks (even just a FIFO queue at first). Hard-code it to a single thread.
* Write a couple of trivial actors to test: e.g., an actor that waits on a timer or on a manually signaled promise and then returns a value. Verify that `wait()`/`await` works as expected (the actor doesn’t block others).

**Phase 2: Event Loop and Scheduling** – *Build out the cooperative scheduler with priorities.*

* Expand the event loop to support task prioritization. Define a range of priority levels (you might start with just “default” and add more later). Internally, maintain a priority queue (min-heap or buckets) for ready tasks.
* Implement the mechanism for an actor to yield control. If using native coroutines, you might need to use something like yielding to an executor or awaiting a special `yield()` future. If custom, implement `yield()` as an actor that simply posts the remainder of the actor’s work back onto the queue.
* Ensure that when a future is not ready, the actor’s continuation is properly enqueued to resume later. In native coroutine terms, this might mean providing a custom `awaiter` for the Future type that registers a continuation.
* Add instrumentation: e.g., count how many tasks are in queue, measure loop latency (maybe not critical now but useful).
* Test with scenarios like: spawn 100 actors that each wait on a timer or an externally fulfilled promise; ensure none starve and all complete. Also test explicit yields by making an actor that does a lot of work (like increments in a loop) but calls `yield()` periodically, and have another actor running concurrently, verifying that both make progress.

**Phase 3: Timers and Clock** – *Introduce time-based waits and a controllable clock.*

* Implement a timer system. In real mode, use an OS or library timer to signal when time has elapsed. In simulation mode, implement a simulated clock and a structure to schedule wake-ups.
* Design an interface for getting the current time (`now()`) and for scheduling a future for a later time (`delay(seconds)` returning a Future<Void>).
* Integrate timers with the event loop: e.g., in each loop iteration, check if any timer’s time has arrived (or in simulation, jump time to next event).
* Test that `wait(delay(x))` works – an actor waiting on a delay should resume after x seconds (test both real time and a toggled simulation mode where you manually drive the clock).
* For simulation determinism, make sure you can freeze time advancement except via the simulation controller.

**Phase 4: Asynchronous I/O Integration** – *Add network and disk operations as futures.*

* Choose how to do async network I/O. Possibly integrate an existing async library (ensuring single-threaded callback usage). For example, in C++ one could use Boost.ASIO’s async ops but configure it to not spawn threads and to only run when polled.
* Implement basic network primitives: e.g., an async TCP connect (returns Future<Connection>), async send (Future<Void> for completion), async receive (Future<Bytes>).
* Implement a simple RPC or message passing on top: maybe define a `Endpoint` concept where one side does `Future<Reply> sendRequest(Request)` and the other does `Request req = waitNext(endpointStream)`.
* Similarly, for disk: perhaps start with async file read/write. On Unix, could use an IO thread or `io_uring` with user-space completion events.
* Ensure all these I/O completions go through the event loop’s queue (so that completions are processed in the deterministic order). Likely by hooking into the same mechanism as futures (when an OS event completes, schedule the corresponding promise fulfillment as a ready task).
* This phase might involve multi-threading *only* for the actual OS waits (like an IO thread), but all callbacks back into the main scheduler must be serialized.
* Test by doing things like: start a server actor that echos messages, start a client actor that sends requests and waits for responses, verify ordering and correctness. Also test file reads if applicable (maybe read from a file and verify data).
* Also test what happens on I/O errors (simulate a network error and see that the future throws).

**Phase 5: Deterministic Simulation Mode** – *Build the simulation environment.*

* Create a global flag or mode for simulation. In simulation, replace the network layer with a fake: maintain in-memory queues for messages instead of real sockets. Make all disk operations either no-ops or operate on a memory data structure.
* Most importantly, implement the simulated clock event loop: rather than tying into real time or actual blocking, have a loop that picks the next scheduled ready time. Possibly unify this with the main scheduler using a single priority queue for both ready tasks and timed tasks.
* Decide on how to represent multiple processes in simulation. Perhaps simulate N processes by giving each actor an attribute of which “process” it belongs to and writing the network layer to only deliver messages if both endpoints are “up” and not partitioned.
* Implement fault injection toggles: e.g., a global `buggify` boolean map that can enable certain faults. Provide a `Buggify(int id)` macro that does `if (g_random->random01() < 0.05 && isBugEnabled(id)) { ... }`. Those conditions can throw errors or alter behavior when triggered.
* Develop a few sample workloads: e.g., a workload that kills and restarts a “process” actor, or one that randomly delays messages. Use these to test the simulation’s ability to simulate chaos.
* Run multiple simulation iterations with a fixed seed and verify determinism: with the same seed, you should get identical sequences of events (you might need to log sequences or outcomes to confirm).
* Also test that real mode and simulation mode produce the same *logical* results in a simple scenario (e.g., a ping-pong message exchange) even if the ordering of internal events differs – they should in fact be the same order if seed is controlling randomness.

**Phase 6: Error Handling and Propagation** – *Solidify the error model.*

* Define an `Error` or `Exception` type to represent failures. Decide how exceptions propagate. If using a language with exceptions, integrate it such that a failed future triggers an exception on await.
* Implement the automatic cancellation: e.g., use weak references or finalizers to cancel pending tasks. In C++, you might implement Future’s destructor to propagate cancellation signals. In a GC language, you might rely on explicit cancellation tokens.
* Write tests for cancellation: spawn an actor that itself spawns a child actor. Drop the parent’s future and ensure the child gets cancelled (maybe have the child do something observable only if not cancelled). Or use a timeout actor to cancel a slow operation and ensure no leftover activity.
* Ensure that catching exceptions in actors works and that retry logic can be implemented. Write a test actor that throws and catches its own exception (like simulate a transient error and handle it).
* Also define a set of error codes for known conditions (like a generic `operation_cancelled`, `timed_out`, etc.) and ensure they compare correctly (FDB’s Error class has a code).

**Phase 7: Advanced Actor Patterns and Library** – *Enhance the API for usability.*

* Add convenience functions or syntactic sugar for common patterns: e.g., a `waitForAll(vector<Future<T>>)` that returns Future\<vector<T>> when all are ready (like join all). Or `waitForAny` (like choose).
* Provide constructs for streams (if not already): a `PromiseStream<T>` and `FutureStream<T>` abstraction for producing multiple values.
* Implement at least one example interface using streams to confirm that the pattern from FDB works (like a simple key-value server interface with get/put as separate streams).
* Add a supervision or grouping mechanism if planned: maybe allow naming actors or grouping them for debugging.
* This is also a good time to flesh out documentation for how to write actors, how cancellation works, etc., as the API should be stabilizing.

**Phase 8: Testing and Simulation at Scale** – *Thoroughly test with complex scenarios.*

* Create multiple complex test workloads: transactions (simulate something like a mini-FDB transaction), fault injection scenarios (e.g., memory fill then recovery), concurrent data structure modifications by many actors, etc. Use these to stress test the framework.
* Run long simulation tests with random seeds to see if any deadlocks or hangs occur (this could catch issues in scheduling).
* Possibly integrate a fuzz tester for the actor compiler if a custom one is used (similar to what FDB did to validate the transformation).
* Measure performance in microbenchmarks: how many future operations per second, context switch overhead, etc. Compare with FDB’s Flow if possible to ensure the new one is in the same ballpark or better.

**Phase 9: Performance Optimization and Polishing** – *Optimize and refine.*

* Based on profiling, optimize hot paths: e.g., reduce allocations (use object pools for actor states or reuse allocated buffers for networking).
* Fine-tune the priority scheduling (maybe implement the priority aging like FDB does: tasks increase in priority as they wait).
* Ensure the event loop can handle burst loads without excessive latency; possibly implement batching of events (like process a batch of I/O events then yield).
* Complete any missing pieces: e.g., secure network (TLS) support if needed, integration with OS signals or external events, etc.
* Add more logging inside the framework: e.g., log when tasks are slow (`SlowTask` analog), log actor create/destroy if debugging is enabled, etc.

**Phase 10: Production Hardening and Documentation** – *Make it production-ready.*

* Write thorough documentation for the framework: API references, design overview, guides for determinism, etc.
* Possibly run the framework in a real service (maybe reimplement a small part of FDB or another system using it) to see it in action under production conditions.
* Address any memory leaks or stability issues observed.
* If open-sourcing or sharing, set up CI with simulation tests to guard regressions.
* Conduct code reviews or security audits especially if using it for handling external inputs (ensuring no deadlock or overflow on malicious patterns, etc.)

By following these phases, we build confidence step by step: first the fundamental concurrency, then integrating external interactions, then layering determinism and testing on top. At each stage, tests validate that we haven’t broken the determinism or correctness.

It’s worth noting that while doing this, one should frequently compare with the known behaviors of the original Flow (as documented and as can be observed in FDB code/tests) to ensure fidelity. References like the FoundationDB forums and documentation cited throughout this document provide guidance on expected behaviors (e.g., cancellation semantics, prioritization rules, etc.). The result of this roadmap, if executed, would be a new Flow-like framework that captures the essence of the original – high-concurrency actors with precise control over execution order and failures – while being more accessible and extensible for future needs.